# -*- coding: utf-8 -*-
"""Credit card fraud detection model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ML8uXsEQxFtAc9mNNSRbBqIK2sF-L5LG
"""

#Import required variables
import numpy as np #arrays
import pandas as pd #data processing
from sklearn.model_selection import train_test_split #split for examples and results
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score #accuracy

# loading dataset to a Pandas DataFrame
credit_card_data = pd.read_csv('/content/creditcard.csv')

#first 5 rows
credit_card_data.head()

#last 5 rows
credit_card_data.tail()

#useless step here but do it anyway - gives you some more info on any blatant fraud
credit_card_data.info()

#rows,columns
credit_card_data.shape

#check for any missing values
credit_card_data.isnull()

#make life easier for yourself with easier representation
credit_card_data.isnull().sum()

#Segregating Legitimate and fraudulent transcations
credit_card_data['Class'].value_counts()

# 0 --> Legitimate transactions
# 1 --> Fradulent transcations
#Dataset is unbalanced so gotta fix that - El Fix(line 24)

#To fix --> separate Legit and Fraud transactions
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]
print(legit.shape)
print(fraud.shape)

#get stats for each
legit.Amount.describe()

fraud.Amount.describe()

#compare values for legit and fraud - notice that legit transactions are mostly in and around 0, while fraud transactions are that much more varied. This is
#easily distinguishable in this scenario but may not be in others - be vary of this.
credit_card_data.groupby('Class').mean()

# El Fix - build a sample dataset that has almost the same number of fradulent transactions to legit ones - but dont just pick out 500 legit
# ones yourself, get the computer to do it

legit_sample = legit.sample(n=500)

# Concatenate/Join the 2 dataframes
new_data = pd.concat([legit_sample, fraud], axis=0)

new_data.head()

new_data.tail()

new_data['Class'].value_counts()

new_data.groupby('Class').mean()

# Splitting the data into Features & Targets
x = new_data.drop(columns= 'Class', axis=1)
y = new_data['Class']

print(x)

print(y)

#Split data into train and testing data
x_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.2) #stratify=y, random_state=2

print(x.shape,x_train.shape, x_test.shape)

# Model Training with Logistic Regression
model = LogisticRegression()

#give the model the training data to get it up to speed
model.fit(x_train, y_train)

#Model Evaluation

#Accuracy Score for train and test done separately
  #Train data
x_train_prediction = model.predict(x_train)
training_data_accuracy = accuracy_score(x_train_prediction, y_train)

print('Accuracy on training data:', training_data_accuracy )

#Test data
x_test_prediction = model.predict(x_test)
test_data_accuracy = accuracy_score(x_test_prediction, y_test)

print('Accuracy on test data:', test_data_accuracy )

